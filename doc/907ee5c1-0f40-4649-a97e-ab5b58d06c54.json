{
    "summary": "This code generates Baidu search URLs, retrieves web page content, and parses data using requests library. It extracts titles, sub-urls, abstracts, and text from pages, handles exceptions, and stores data in a JSON file.",
    "details": [
        {
            "comment": "The code defines functions for generating Baidu search URLs and retrieving page content using the requests library. It also sets headers to mimic a browser request and handles potential exceptions when making the request.",
            "location": "\"/media/root/Prima/works/generated_docs/lazero_doc/src/bootstrap/keller/src/the_real_wheel.py\":0-41",
            "content": "import requests\nfrom dbM import up\nimport urllib.parse\nimport time\nfrom requests.exceptions import RequestException\nfrom urllib.parse import urljoin\nfrom lxml import etree\nimport re\nimport json\n# \u767e\u5ea6\u641c\u7d22\u63a5\u53e3\ndef format_url(url, params: dict = None) -> str:\n    query_str = urllib.parse.urlencode(params)\n    return f'{ url }?{ query_str }'\ndef get_url(keyword):\n    params = {\n        'wd': str(keyword)\n    }\n    url = \"https://www.baidu.com/s\"\n    url = format_url(url, params)\n    # print(url)\n    return url\ndef get_page(url):\n    try:\n        headers = {\n            'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Safari/537.36',\n            'accept-language': 'zh-CN,zh;q=0.9',\n            'cache-control': 'max-age=0',\n            'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8'\n        }\n        response = requests.get(url=url, headers=headers)\n        # \u66f4\u6539\u7f16\u7801\u65b9\u5f0f\uff0c\u5426\u5219\u4f1a\u51fa\u73b0\u4e71\u7801\u7684\u60c5\u51b5\n        # but it is no faster than this."
        },
        {
            "comment": "This code defines a function 'parse_page' to scrape data from a web page. It sends a request, handles exceptions, extracts title, sub_url, and abstract for each item on the page, and returns the text if the status code is 200. The function iterates over pages and items on each page, adjusting the range of items based on the current page number.",
            "location": "\"/media/root/Prima/works/generated_docs/lazero_doc/src/bootstrap/keller/src/the_real_wheel.py\":42-77",
            "content": "        response.encoding = \"utf-8\"\n        print(response.status_code)\n        # print(response.text)\n        if response.status_code == 200:\n            return response.text\n        return None\n    except RequestException:\n        return None\ndef parse_page(url, page):\n    for i in range(1, int(page)+1):\n        print(\"\u6b63\u5728\u722c\u53d6\u7b2c{}\u9875....\".format(i))\n        title = \"\"\n        sub_url = \"\"\n        abstract = \"\"\n        flag = 11\n        if i == 1:\n            flag = 10\n        html = get_page(url)\n        content = etree.HTML(html)\n        for j in range(1, flag):\n            data = {}\n            res_title = content.xpath(\n                '//*[@id=\"%d\"]/h3/a' % ((i - 1) * 10 + j))\n            if res_title:\n                title = res_title[0].xpath('string(.)')\n            sub_url = content.xpath(\n                '//*[@id=\"%d\"]/h3/a/@href' % ((i - 1) * 10 + j))\n            if sub_url:\n                sub_url = sub_url[0]\n            res_abstract = content.xpath(\n                '//*[@id=\"%d\"]/div[@class=\"c-abstract\"]' % ((i-1)*10+j))"
        },
        {
            "comment": "This code is searching for abstracts on a web page, extracting titles and sub-urls, and yielding data. It handles multiple pages by finding the next page link based on a flag. The user inputs keyword and desired page number. If there are no more pages, it prints \"\u65e0\u66f4\u591a\u9875\u9762\uff01\uff5e\" and returns.",
            "location": "\"/media/root/Prima/works/generated_docs/lazero_doc/src/bootstrap/keller/src/the_real_wheel.py\":78-104",
            "content": "            if res_abstract:\n                abstract = res_abstract[0].xpath('string(.)')\n            else:\n                res_abstract = content.xpath(\n                    '//*[@id=\"%d\"]/div/div[2]/div[@class=\"c-abstract\"]' % ((i-1)*10+j))\n                if res_abstract:\n                    abstract = res_abstract[0].xpath('string(.)')\n                    # res_abstract = content.xpath('//*[@id=\"%d\"]/div/div[2]/p[1]'%((i-1)*10+j))\n            # if not abstract:\n            #     abstract = content.xpath('//*[@id=\"%d\"]/div/div[2]/p[1]'%((i-1)*10+j))[0].xpath('string(.)')\n            data['title'] = title\n            data['sub_url'] = sub_url\n            data['abstract'] = abstract\n            rel_url = content.xpath('//*[@id=\"page\"]/a[{}]/@href'.format(flag))\n            if rel_url:\n                url = urljoin(url, rel_url[0])\n            else:\n                print(\"\u65e0\u66f4\u591a\u9875\u9762\uff01\uff5e\")\n                return\n            yield data\ndef main():\n    keyword = input(\"\u8f93\u5165\u5173\u952e\u5b57:\")\n    page = input(\"\u8f93\u5165\u67e5\u627e\u9875\u6570:\")\n    url = get_url(keyword)"
        },
        {
            "comment": "This code reads data from a page and stores it in a JSON file named \"data.json\". It uses the parse_page function to retrieve the data, then iterates over the results and writes each result into the file after updating a timestamp and incrementing a file counter. The code also has a print statement that displays each result, but it is commented out with an explanation of what to do if we want to use the result. The code runs the main() function when executed as the main module.",
            "location": "\"/media/root/Prima/works/generated_docs/lazero_doc/src/bootstrap/keller/src/the_real_wheel.py\":106-120",
            "content": "    results = parse_page(url, page)\n    # \u5199\u5165\u6587\u4ef6\n    # file = open(\"data.json\", 'w+', encoding='utf-8')\n    file = 0\n    t = int(time.time())\n    for result in results:\n        up(t, file, keyword, result)\n        file += 1\n        # waht if we want to use the result?\n        print(result)\n    #     file.write(json.dumps(result, indent=2, ensure_ascii=False))\nif __name__ == '__main__':\n    main()"
        }
    ]
}