{
    "summary": "The code imports libraries, defines a URL formatting function, and gets Baidu search URLs. It extracts titles, sub-urls, and abstracts from paginated results, parses webpage data, writes to \"data.json\", updates file number, and prints results on console for debugging or usage.",
    "details": [
        {
            "comment": "This code imports necessary libraries, defines a function to format URLs, gets a Baidu search URL based on a keyword, and sets headers for making requests.",
            "location": "\"/media/root/Prima/works/generated_docs/lazero_doc/src/bootstrap/legacy/concentration/new_toys/batch_wheel.py\":0-42",
            "content": "import requests\nfrom dbM import up\nimport urllib.parse\nimport time\nfrom requests.exceptions import RequestException\nfrom urllib.parse import urljoin\nfrom lxml import etree\nimport re\nimport json\n# disable browser protocol.\n# \u767e\u5ea6\u641c\u7d22\u63a5\u53e3\ndef format_url(url, params: dict = None) -> str:\n    query_str = urllib.parse.urlencode(params)\n    return f'{ url }?{ query_str }'\ndef get_url(keyword):\n    params = {\n        'wd': str(keyword)\n    }\n    url = \"https://www.baidu.com/s\"\n    url = format_url(url, params)\n    # print(url)\n    return url\ndef get_page(url):\n    try:\n        headers = {\n            'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Safari/537.36',\n            'accept-language': 'zh-CN,zh;q=0.9',\n            'cache-control': 'max-age=0',\n            'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8'\n        }\n        response = requests.get(url=url, headers=headers)\n        # \u66f4\u6539\u7f16\u7801\u65b9\u5f0f\uff0c\u5426\u5219\u4f1a\u51fa\u73b0\u4e71\u7801\u7684\u60c5\u51b5\n        response.encoding = \"utf-8\""
        },
        {
            "comment": "This function gets the content of a page and parses it to extract titles, sub-urls, and abstracts for each item on the page. It handles error cases by returning None when an unexpected RequestException occurs. The code is designed to work with paginated results where each page contains 10 items. The variable 'i' represents the current page number, while 'j' ranges from 1 to 11 (or 10 on the first page) for each item on a given page. This function is called in a loop to process multiple pages of data.",
            "location": "\"/media/root/Prima/works/generated_docs/lazero_doc/src/bootstrap/legacy/concentration/new_toys/batch_wheel.py\":43-78",
            "content": "        print(response.status_code)\n        # print(response.text)\n        if response.status_code == 200:\n            return response.text\n        return None\n    except RequestException:\n        return None\ndef parse_page(url, page):\n    for i in range(1, int(page)+1):\n        print(\"\u6b63\u5728\u722c\u53d6\u7b2c{}\u9875....\".format(i))\n        title = \"\"\n        sub_url = \"\"\n        abstract = \"\"\n        flag = 11\n        if i == 1:\n            flag = 10\n        html = get_page(url)\n        content = etree.HTML(html)\n        for j in range(1, flag):\n            data = {}\n            res_title = content.xpath(\n                '//*[@id=\"%d\"]/h3/a' % ((i - 1) * 10 + j))\n            if res_title:\n                title = res_title[0].xpath('string(.)')\n            sub_url = content.xpath(\n                '//*[@id=\"%d\"]/h3/a/@href' % ((i - 1) * 10 + j))\n            if sub_url:\n                sub_url = sub_url[0]\n            res_abstract = content.xpath(\n                '//*[@id=\"%d\"]/div[@class=\"c-abstract\"]' % ((i-1)*10+j))\n            if res_abstract:"
        },
        {
            "comment": "This code appears to be parsing data from a website. It finds the title, abstract, and related URLs for each page based on a given keyword and page number. If there are no more pages, it prints \"\u65e0\u66f4\u591a\u9875\u9762\uff01\uff5e\" and returns.",
            "location": "\"/media/root/Prima/works/generated_docs/lazero_doc/src/bootstrap/legacy/concentration/new_toys/batch_wheel.py\":79-105",
            "content": "                abstract = res_abstract[0].xpath('string(.)')\n            else:\n                res_abstract = content.xpath(\n                    '//*[@id=\"%d\"]/div/div[2]/div[@class=\"c-abstract\"]' % ((i-1)*10+j))\n                if res_abstract:\n                    abstract = res_abstract[0].xpath('string(.)')\n                    # res_abstract = content.xpath('//*[@id=\"%d\"]/div/div[2]/p[1]'%((i-1)*10+j))\n            # if not abstract:\n            #     abstract = content.xpath('//*[@id=\"%d\"]/div/div[2]/p[1]'%((i-1)*10+j))[0].xpath('string(.)')\n            data['title'] = title\n            data['sub_url'] = sub_url\n            data['abstract'] = abstract\n            rel_url = content.xpath('//*[@id=\"page\"]/a[{}]/@href'.format(flag))\n            if rel_url:\n                url = urljoin(url, rel_url[0])\n            else:\n                print(\"\u65e0\u66f4\u591a\u9875\u9762\uff01\uff5e\")\n                return\n            yield data\ndef main(keyword, page):\n    # keyword = input(\"\u8f93\u5165\u5173\u952e\u5b57:\")\n    #    page = input(\"\u8f93\u5165\u67e5\u627e\u9875\u6570:\")\n    # page = 2\n    url = get_url(keyword)"
        },
        {
            "comment": "This code is parsing webpage data using the \"parse_page\" function, then writing each result to a file named \"data.json\". It keeps track of the current file number and updates it after each write operation. The results are printed on the console for potential usage or debugging purposes.",
            "location": "\"/media/root/Prima/works/generated_docs/lazero_doc/src/bootstrap/legacy/concentration/new_toys/batch_wheel.py\":107-121",
            "content": "    results = parse_page(url, page)\n    # \u5199\u5165\u6587\u4ef6\n    # file = open(\"data.json\", 'w+', encoding='utf-8')\n    file = 0\n    t = int(time.time())\n    for result in results:\n        up(t, file, keyword, result)\n        file += 1\n        # waht if we want to use the result?\n        print(result)\n    #     file.write(json.dumps(result, indent=2, ensure_ascii=False))\n# if __name__ == '__main__':\n#     main()"
        }
    ]
}