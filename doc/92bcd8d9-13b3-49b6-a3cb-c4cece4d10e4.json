{
    "summary": "This code uses Baidu search API to format and retrieve URLs, retrieves HTML content with specified parameters, and stores data in \"data.json\". It displays results with timestamps, handles multiple pages, and finds relative URLs.",
    "details": [
        {
            "comment": "This code defines functions for formatting and retrieving URLs using Baidu search API. It imports necessary libraries, including requests and lxml. The get_url function takes a keyword, formats it into a URL with the Baidu search parameters, and returns it. The get_page function sends an HTTP GET request to the generated URL using custom headers for user-agent and accept-language, and retrieves the webpage content as response.",
            "location": "\"/media/root/Prima/works/generated_docs/lazero_doc/src/bootstrap/legacy/concentration/new_toys/the_real_wheel.py\":0-41",
            "content": "import requests\nfrom dbM import up\nimport urllib.parse\nimport time\nfrom requests.exceptions import RequestException\nfrom urllib.parse import urljoin\nfrom lxml import etree\nimport re\nimport json\n# \u767e\u5ea6\u641c\u7d22\u63a5\u53e3\ndef format_url(url, params: dict = None) -> str:\n    query_str = urllib.parse.urlencode(params)\n    return f'{ url }?{ query_str }'\ndef get_url(keyword):\n    params = {\n        'wd': str(keyword)\n    }\n    url = \"https://www.baidu.com/s\"\n    url = format_url(url, params)\n    # print(url)\n    return url\ndef get_page(url):\n    try:\n        headers = {\n            'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Safari/537.36',\n            'accept-language': 'zh-CN,zh;q=0.9',\n            'cache-control': 'max-age=0',\n            'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8'\n        }\n        response = requests.get(url=url, headers=headers)\n        # \u66f4\u6539\u7f16\u7801\u65b9\u5f0f\uff0c\u5426\u5219\u4f1a\u51fa\u73b0\u4e71\u7801\u7684\u60c5\u51b5\n        # but it is no faster than this."
        },
        {
            "comment": "The function `get_page` retrieves the HTML content of a URL. The variable `flag` determines how many elements are parsed from each page. For each page, it extracts titles, URLs, and abstracts from the HTML using XPath expressions. If a status code 200 is returned, the function returns the parsed data; otherwise, it returns None.",
            "location": "\"/media/root/Prima/works/generated_docs/lazero_doc/src/bootstrap/legacy/concentration/new_toys/the_real_wheel.py\":42-77",
            "content": "        response.encoding = \"utf-8\"\n        print(response.status_code)\n        # print(response.text)\n        if response.status_code == 200:\n            return response.text\n        return None\n    except RequestException:\n        return None\ndef parse_page(url, page):\n    for i in range(1, int(page)+1):\n        print(\"\u6b63\u5728\u722c\u53d6\u7b2c{}\u9875....\".format(i))\n        title = \"\"\n        sub_url = \"\"\n        abstract = \"\"\n        flag = 11\n        if i == 1:\n            flag = 10\n        html = get_page(url)\n        content = etree.HTML(html)\n        for j in range(1, flag):\n            data = {}\n            res_title = content.xpath(\n                '//*[@id=\"%d\"]/h3/a' % ((i - 1) * 10 + j))\n            if res_title:\n                title = res_title[0].xpath('string(.)')\n            sub_url = content.xpath(\n                '//*[@id=\"%d\"]/h3/a/@href' % ((i - 1) * 10 + j))\n            if sub_url:\n                sub_url = sub_url[0]\n            res_abstract = content.xpath(\n                '//*[@id=\"%d\"]/div[@class=\"c-abstract\"]' % ((i-1)*10+j))"
        },
        {
            "comment": "The code is retrieving data from a web page, specifically the title, sub_url, and abstract. It then finds the relative URL to move on to the next page. If there are no more pages, it prints \"\u65e0\u66f4\u591a\u9875\u9762\uff01\uff5e\" and returns.",
            "location": "\"/media/root/Prima/works/generated_docs/lazero_doc/src/bootstrap/legacy/concentration/new_toys/the_real_wheel.py\":78-103",
            "content": "            if res_abstract:\n                abstract = res_abstract[0].xpath('string(.)')\n            else:\n                res_abstract = content.xpath(\n                    '//*[@id=\"%d\"]/div/div[2]/div[@class=\"c-abstract\"]' % ((i-1)*10+j))\n                if res_abstract:\n                    abstract = res_abstract[0].xpath('string(.)')\n                    # res_abstract = content.xpath('//*[@id=\"%d\"]/div/div[2]/p[1]'%((i-1)*10+j))\n            # if not abstract:\n            #     abstract = content.xpath('//*[@id=\"%d\"]/div/div[2]/p[1]'%((i-1)*10+j))[0].xpath('string(.)')\n            data['title'] = title\n            data['sub_url'] = sub_url\n            data['abstract'] = abstract\n            rel_url = content.xpath('//*[@id=\"page\"]/a[{}]/@href'.format(flag))\n            # what the heck? renewed the mechanism?\n            if rel_url:\n                url = urljoin(url, rel_url[0])\n            else:\n                print(\"\u65e0\u66f4\u591a\u9875\u9762\uff01\uff5e\")\n                return\n            yield data\ndef main():\n    keyword = input(\"\u8f93\u5165\u5173\u952e\u5b57:\")"
        },
        {
            "comment": "This code prompts the user to enter a page number, retrieves and parses data from a URL based on a keyword, and then writes each result into a file named \"data.json\". The results are printed, but could potentially be used for further processing. The time is recorded with every result, and the file variable keeps track of the current result's position in the file.",
            "location": "\"/media/root/Prima/works/generated_docs/lazero_doc/src/bootstrap/legacy/concentration/new_toys/the_real_wheel.py\":104-120",
            "content": "    page = input(\"\u8f93\u5165\u67e5\u627e\u9875\u6570:\")\n    url = get_url(keyword)\n    results = parse_page(url, page)\n    # \u5199\u5165\u6587\u4ef6\n    # file = open(\"data.json\", 'w+', encoding='utf-8')\n    file = 0\n    t = int(time.time())\n    for result in results:\n        up(t, file, keyword, result)\n        file += 1\n        # waht if we want to use the result?\n        print(result)\n    #     file.write(json.dumps(result, indent=2, ensure_ascii=False))\nif __name__ == '__main__':\n    main()"
        }
    ]
}